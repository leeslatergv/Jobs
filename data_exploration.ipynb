{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Import data from the webscraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 66 webscrape CSV files\n",
      "Reference headers from webscrape_2023-08-15_09-20-02.csv: ['title', 'company', 'location', 'salary', 'description']\n",
      "\n",
      "All files have identical column headers\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Get the parent directory path\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# Find all files in the parent directory matching the webscrape pattern\n",
    "csv_pattern = os.path.join(parent_dir, \"webscrape_*.csv\")\n",
    "csv_files = glob.glob(csv_pattern)\n",
    "\n",
    "# Store column headers for each file\n",
    "all_headers = {}\n",
    "consistent = True\n",
    "first_file = None\n",
    "\n",
    "print(f\"Found {len(csv_files)} webscrape CSV files\")\n",
    "\n",
    "# Loop through each file and get headers\n",
    "for file in csv_files:\n",
    "    filename = os.path.basename(file)\n",
    "    \n",
    "    # Just read the headers without loading entire file\n",
    "    headers = pd.read_csv(file, nrows=0).columns.tolist()\n",
    "    all_headers[filename] = headers\n",
    "    \n",
    "    # Store first file's headers as reference\n",
    "    if first_file is None:\n",
    "        first_file = filename\n",
    "        reference_headers = headers\n",
    "        print(f\"Reference headers from {first_file}: {reference_headers}\")\n",
    "    # Compare current file with reference\n",
    "    elif headers != reference_headers:\n",
    "        consistent = False\n",
    "        print(f\"\\nMISMATCH in {filename}:\")\n",
    "        \n",
    "        # Find and show differences\n",
    "        missing = set(reference_headers) - set(headers)\n",
    "        extra = set(headers) - set(reference_headers)\n",
    "        \n",
    "        if missing:\n",
    "            print(f\"  Missing columns: {missing}\")\n",
    "        if extra:\n",
    "            print(f\"  Extra columns: {extra}\")\n",
    "\n",
    "# Print final result\n",
    "if consistent:\n",
    "    print(\"\\nAll files have identical column headers\")\n",
    "else:\n",
    "    print(\"\\nWarning: Column headers differ between files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. Add date and date-times so we can identify where each job came from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded webscrape_2023-08-15_09-20-02.csv with 3923 rows, scraped on 2023-08-15 09:20:02\n",
      "Loaded webscrape_2023-08-23_09-55-48.csv with 4151 rows, scraped on 2023-08-23 09:55:48\n",
      "Loaded webscrape_2023-08-30_10-13-36.csv with 3897 rows, scraped on 2023-08-30 10:13:36\n",
      "Loaded webscrape_2023-09-07_09-32-27.csv with 4266 rows, scraped on 2023-09-07 09:32:27\n",
      "Loaded webscrape_2023-09-27_11-14-05.csv with 3458 rows, scraped on 2023-09-27 11:14:05\n",
      "Loaded webscrape_2023-10-04_09-34-49.csv with 3712 rows, scraped on 2023-10-04 09:34:49\n",
      "Loaded webscrape_2023-10-11_07-59-43.csv with 3362 rows, scraped on 2023-10-11 07:59:43\n",
      "Loaded webscrape_2023-10-18_09-22-00.csv with 3899 rows, scraped on 2023-10-18 09:22:00\n",
      "Loaded webscrape_2023-11-01_08-31-18.csv with 4629 rows, scraped on 2023-11-01 08:31:18\n",
      "Loaded webscrape_2023-11-08_10-36-53.csv with 1859 rows, scraped on 2023-11-08 10:36:53\n",
      "Loaded webscrape_2023-11-22_10-11-56.csv with 2762 rows, scraped on 2023-11-22 10:11:56\n",
      "Loaded webscrape_2023-11-29_10-05-19.csv with 2931 rows, scraped on 2023-11-29 10:05:19\n",
      "Loaded webscrape_2023-12-06_08-37-21.csv with 3958 rows, scraped on 2023-12-06 08:37:21\n",
      "Loaded webscrape_2023-12-13_22-50-29.csv with 4141 rows, scraped on 2023-12-13 22:50:29\n",
      "Loaded webscrape_2023-12-20_20-54-26.csv with 3775 rows, scraped on 2023-12-20 20:54:26\n",
      "Loaded webscrape_2024-01-03_08-30-45.csv with 3193 rows, scraped on 2024-01-03 08:30:45\n",
      "Loaded webscrape_2024-01-10_15-49-59.csv with 4081 rows, scraped on 2024-01-10 15:49:59\n",
      "Loaded webscrape_2024-01-17_08-29-40.csv with 3630 rows, scraped on 2024-01-17 08:29:40\n",
      "Loaded webscrape_2024-01-31_08-48-04.csv with 3239 rows, scraped on 2024-01-31 08:48:04\n",
      "Loaded webscrape_2024-02-07_09-02-53.csv with 3268 rows, scraped on 2024-02-07 09:02:53\n",
      "Loaded webscrape_2024-02-14_09-27-41.csv with 3427 rows, scraped on 2024-02-14 09:27:41\n",
      "Loaded webscrape_2024-02-21_08-09-31.csv with 3644 rows, scraped on 2024-02-21 08:09:31\n",
      "Loaded webscrape_2024-03-13_08-28-35.csv with 3050 rows, scraped on 2024-03-13 08:28:35\n",
      "Loaded webscrape_2024-03-20_08-11-25.csv with 3491 rows, scraped on 2024-03-20 08:11:25\n",
      "Loaded webscrape_2024-03-27_08-16-53.csv with 2945 rows, scraped on 2024-03-27 08:16:53\n",
      "Loaded webscrape_2024-04-10_08-45-14.csv with 3504 rows, scraped on 2024-04-10 08:45:14\n",
      "Loaded webscrape_2024-04-17_08-08-37.csv with 4330 rows, scraped on 2024-04-17 08:08:37\n",
      "Loaded webscrape_2024-04-24_08-41-09.csv with 3500 rows, scraped on 2024-04-24 08:41:09\n",
      "Loaded webscrape_2024-05-01_09-20-32.csv with 2978 rows, scraped on 2024-05-01 09:20:32\n",
      "Loaded webscrape_2024-05-08_08-47-15.csv with 3199 rows, scraped on 2024-05-08 08:47:15\n",
      "Loaded webscrape_2024-05-15_08-05-44.csv with 2871 rows, scraped on 2024-05-15 08:05:44\n",
      "Loaded webscrape_2024-05-22_11-20-20.csv with 2951 rows, scraped on 2024-05-22 11:20:20\n",
      "Loaded webscrape_2024-05-29_08-28-04.csv with 2248 rows, scraped on 2024-05-29 08:28:04\n",
      "Loaded webscrape_2024-06-05_08-36-42.csv with 2983 rows, scraped on 2024-06-05 08:36:42\n",
      "Loaded webscrape_2024-06-12_08-43-59.csv with 2814 rows, scraped on 2024-06-12 08:43:59\n",
      "Loaded webscrape_2024-06-19_08-37-29.csv with 2501 rows, scraped on 2024-06-19 08:37:29\n",
      "Loaded webscrape_2024-06-26_08-33-33.csv with 2691 rows, scraped on 2024-06-26 08:33:33\n",
      "Loaded webscrape_2024-07-03_08-19-16.csv with 2977 rows, scraped on 2024-07-03 08:19:16\n",
      "Loaded webscrape_2024-07-10_08-04-22.csv with 2637 rows, scraped on 2024-07-10 08:04:22\n",
      "Loaded webscrape_2024-07-17_10-30-02.csv with 2365 rows, scraped on 2024-07-17 10:30:02\n",
      "Loaded webscrape_2024-07-31_10-18-59.csv with 2647 rows, scraped on 2024-07-31 10:18:59\n",
      "Loaded webscrape_2024-08-14_08-46-04.csv with 2732 rows, scraped on 2024-08-14 08:46:04\n",
      "Loaded webscrape_2024-08-21_08-24-07.csv with 2598 rows, scraped on 2024-08-21 08:24:07\n",
      "Loaded webscrape_2024-08-28_08-22-07.csv with 2210 rows, scraped on 2024-08-28 08:22:07\n",
      "Loaded webscrape_2024-09-04_08-34-00.csv with 3159 rows, scraped on 2024-09-04 08:34:00\n",
      "Loaded webscrape_2024-10-09_08-45-47.csv with 2568 rows, scraped on 2024-10-09 08:45:47\n",
      "Loaded webscrape_2024-10-16_08-19-38.csv with 2374 rows, scraped on 2024-10-16 08:19:38\n",
      "Loaded webscrape_2024-10-23_11-48-05.csv with 3016 rows, scraped on 2024-10-23 11:48:05\n",
      "Loaded webscrape_2024-10-30_08-23-59.csv with 5270 rows, scraped on 2024-10-30 08:23:59\n",
      "Loaded webscrape_2024-11-06_08-26-04.csv with 4360 rows, scraped on 2024-11-06 08:26:04\n",
      "Loaded webscrape_2024-11-13_08-59-33.csv with 4969 rows, scraped on 2024-11-13 08:59:33\n",
      "Loaded webscrape_2024-11-20_08-36-01.csv with 2848 rows, scraped on 2024-11-20 08:36:01\n",
      "Loaded webscrape_2024-11-27_08-28-22.csv with 2656 rows, scraped on 2024-11-27 08:28:22\n",
      "Loaded webscrape_2024-12-04_08-23-03.csv with 2501 rows, scraped on 2024-12-04 08:23:03\n",
      "Loaded webscrape_2024-12-11_10-23-04.csv with 2377 rows, scraped on 2024-12-11 10:23:04\n",
      "Loaded webscrape_2024-12-18_09-13-24.csv with 2585 rows, scraped on 2024-12-18 09:13:24\n",
      "Loaded webscrape_2025-01-08_08-43-27.csv with 1949 rows, scraped on 2025-01-08 08:43:27\n",
      "Loaded webscrape_2025-01-15_09-24-29.csv with 3021 rows, scraped on 2025-01-15 09:24:29\n",
      "Loaded webscrape_2025-01-22_08-47-45.csv with 4559 rows, scraped on 2025-01-22 08:47:45\n",
      "Loaded webscrape_2025-01-29_09-16-29.csv with 5272 rows, scraped on 2025-01-29 09:16:29\n",
      "Loaded webscrape_2025-02-05_07-43-17.csv with 5089 rows, scraped on 2025-02-05 07:43:17\n",
      "Loaded webscrape_2025-02-19_08-33-34.csv with 5565 rows, scraped on 2025-02-19 08:33:34\n",
      "Loaded webscrape_2025-02-26_08-20-19.csv with 5122 rows, scraped on 2025-02-26 08:20:19\n",
      "Loaded webscrape_2025-03-05_08-49-43.csv with 4548 rows, scraped on 2025-03-05 08:49:43\n",
      "Loaded webscrape_2025-03-19_09-48-27.csv with 5496 rows, scraped on 2025-03-19 09:48:27\n",
      "Loaded webscrape_2025-03-26_16-52-18.csv with 4479 rows, scraped on 2025-03-26 16:52:18\n",
      "\n",
      "Processed 66 files\n",
      "\n",
      "Example from webscrape_2023-08-15_09-20-02.csv:\n",
      "  scrape_date     scrape_datetime\n",
      "0  2023-08-15 2023-08-15 09:20:02\n",
      "1  2023-08-15 2023-08-15 09:20:02\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrames with dates extracted from filenames\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Dictionary to store all DataFrames\n",
    "dataframes = {}\n",
    "\n",
    "# Regular expression to extract date and time from filename\n",
    "date_pattern = r'webscrape_(\\d{4}-\\d{2}-\\d{2})_(\\d{2}-\\d{2}-\\d{2})\\.csv'\n",
    "\n",
    "# Process each CSV file\n",
    "for file in csv_files:\n",
    "    filename = os.path.basename(file)\n",
    "    \n",
    "    # Extract date using regex\n",
    "    match = re.search(date_pattern, filename)\n",
    "    if match:\n",
    "        date_str = match.group(1)  # YYYY-MM-DD\n",
    "        time_str = match.group(2).replace('-', ':')  # Convert to HH:MM:SS\n",
    "        datetime_str = f\"{date_str} {time_str}\"\n",
    "        \n",
    "        # Parse to datetime object\n",
    "        file_datetime = datetime.strptime(datetime_str, '%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        # Read the CSV\n",
    "        df = pd.read_csv(file)\n",
    "        \n",
    "        # Add date columns\n",
    "        df['scrape_date'] = file_datetime.date()\n",
    "        df['scrape_datetime'] = file_datetime\n",
    "        \n",
    "        # Store in dictionary\n",
    "        dataframes[filename] = df\n",
    "        \n",
    "        print(f\"Loaded {filename} with {len(df)} rows, scraped on {file_datetime}\")\n",
    "    else:\n",
    "        print(f\"Warning: Couldn't extract date from {filename}\")\n",
    "\n",
    "print(f\"\\nProcessed {len(dataframes)} files\")\n",
    "\n",
    "# Example to verify date columns were added correctly\n",
    "if dataframes:\n",
    "    first_key = list(dataframes.keys())[0]\n",
    "    print(f\"\\nExample from {first_key}:\")\n",
    "    print(dataframes[first_key][['scrape_date', 'scrape_datetime']].head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3. Merge the webscrape CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset created successfully!\n",
      "Total rows: 227,210\n",
      "Total columns: 7\n",
      "\n",
      "Data includes 66 unique dates:\n",
      "  2023-08-15: 3,923 rows\n",
      "  2023-08-23: 4,151 rows\n",
      "  2023-08-30: 3,897 rows\n",
      "  2023-09-07: 4,266 rows\n",
      "  2023-09-27: 3,458 rows\n",
      "  2023-10-04: 3,712 rows\n",
      "  2023-10-11: 3,362 rows\n",
      "  2023-10-18: 3,899 rows\n",
      "  2023-11-01: 4,629 rows\n",
      "  2023-11-08: 1,859 rows\n",
      "  2023-11-22: 2,762 rows\n",
      "  2023-11-29: 2,931 rows\n",
      "  2023-12-06: 3,958 rows\n",
      "  2023-12-13: 4,141 rows\n",
      "  2023-12-20: 3,775 rows\n",
      "  2024-01-03: 3,193 rows\n",
      "  2024-01-10: 4,081 rows\n",
      "  2024-01-17: 3,630 rows\n",
      "  2024-01-31: 3,239 rows\n",
      "  2024-02-07: 3,268 rows\n",
      "  2024-02-14: 3,427 rows\n",
      "  2024-02-21: 3,644 rows\n",
      "  2024-03-13: 3,050 rows\n",
      "  2024-03-20: 3,491 rows\n",
      "  2024-03-27: 2,945 rows\n",
      "  2024-04-10: 3,504 rows\n",
      "  2024-04-17: 4,330 rows\n",
      "  2024-04-24: 3,500 rows\n",
      "  2024-05-01: 2,978 rows\n",
      "  2024-05-08: 3,199 rows\n",
      "  2024-05-15: 2,871 rows\n",
      "  2024-05-22: 2,951 rows\n",
      "  2024-05-29: 2,248 rows\n",
      "  2024-06-05: 2,983 rows\n",
      "  2024-06-12: 2,814 rows\n",
      "  2024-06-19: 2,501 rows\n",
      "  2024-06-26: 2,691 rows\n",
      "  2024-07-03: 2,977 rows\n",
      "  2024-07-10: 2,637 rows\n",
      "  2024-07-17: 2,365 rows\n",
      "  2024-07-31: 2,647 rows\n",
      "  2024-08-14: 2,732 rows\n",
      "  2024-08-21: 2,598 rows\n",
      "  2024-08-28: 2,210 rows\n",
      "  2024-09-04: 3,159 rows\n",
      "  2024-10-09: 2,568 rows\n",
      "  2024-10-16: 2,374 rows\n",
      "  2024-10-23: 3,016 rows\n",
      "  2024-10-30: 5,270 rows\n",
      "  2024-11-06: 4,360 rows\n",
      "  2024-11-13: 4,969 rows\n",
      "  2024-11-20: 2,848 rows\n",
      "  2024-11-27: 2,656 rows\n",
      "  2024-12-04: 2,501 rows\n",
      "  2024-12-11: 2,377 rows\n",
      "  2024-12-18: 2,585 rows\n",
      "  2025-01-08: 1,949 rows\n",
      "  2025-01-15: 3,021 rows\n",
      "  2025-01-22: 4,559 rows\n",
      "  2025-01-29: 5,272 rows\n",
      "  2025-02-05: 5,089 rows\n",
      "  2025-02-19: 5,565 rows\n",
      "  2025-02-26: 5,122 rows\n",
      "  2025-03-05: 4,548 rows\n",
      "  2025-03-19: 5,496 rows\n",
      "  2025-03-26: 4,479 rows\n",
      "\n",
      "Preview of combined dataset:\n",
      "  scrape_date     scrape_datetime  \\\n",
      "0  2023-08-15 2023-08-15 09:20:02   \n",
      "1  2023-08-15 2023-08-15 09:20:02   \n",
      "2  2023-08-15 2023-08-15 09:20:02   \n",
      "3  2023-08-15 2023-08-15 09:20:02   \n",
      "4  2023-08-15 2023-08-15 09:20:02   \n",
      "\n",
      "                                               title  \\\n",
      "0              Average Salary£41,855See More Stats ❯   \n",
      "1                      Advanced Nurse Practitione...   \n",
      "2                                   Registered Nurse   \n",
      "3                      Occupational Therapist Ass...   \n",
      "4                                  An analogue gauge   \n",
      "\n",
      "                                             company  \\\n",
      "0              Receive the newest jobs for this s...   \n",
      "1                      PP ASSOCIATES LTD               \n",
      "2                   EXEMPLAR HEALTH CARE               \n",
      "3              ELYSIUM HEALTHCARE LIMITED        ...   \n",
      "4          Boost your CV\\r\\n        It takes 2 mi...   \n",
      "\n",
      "                                            location  \n",
      "0                                   Create alert      \n",
      "1                              LIVERPOOL, NORTH WEST  \n",
      "2                      ROCHDALE, NORTH WEST, OL16...  \n",
      "3                      WARRINGTON, CHESHIRE, NORT...  \n",
      "4                                                NaN  \n"
     ]
    }
   ],
   "source": [
    "# Merge all DataFrames into one combined dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Check if we have DataFrames to merge\n",
    "if not dataframes:\n",
    "    print(\"No DataFrames to merge!\")\n",
    "else:\n",
    "    # Combine all DataFrames from the dictionary\n",
    "    combined_df = pd.concat(dataframes.values(), ignore_index=True)\n",
    "    \n",
    "    # Basic info about the combined dataset\n",
    "    print(f\"Combined dataset created successfully!\")\n",
    "    print(f\"Total rows: {len(combined_df):,}\")\n",
    "    print(f\"Total columns: {len(combined_df.columns)}\")\n",
    "    \n",
    "    # Show unique scrape dates to verify we have data from different days\n",
    "    unique_dates = combined_df['scrape_date'].unique()\n",
    "    print(f\"\\nData includes {len(unique_dates)} unique dates:\")\n",
    "    for date in sorted(unique_dates):\n",
    "        count = len(combined_df[combined_df['scrape_date'] == date])\n",
    "        print(f\"  {date}: {count:,} rows\")\n",
    "    \n",
    "    # Preview of the combined data\n",
    "    print(\"\\nPreview of combined dataset:\")\n",
    "    # Show date columns first, then a few others\n",
    "    preview_cols = ['scrape_date', 'scrape_datetime'] \n",
    "    # Add a few more columns for preview (adjust as needed)\n",
    "    other_cols = [col for col in combined_df.columns \n",
    "                 if col not in ['scrape_date', 'scrape_datetime']][:3]\n",
    "    preview_cols.extend(other_cols)\n",
    "    \n",
    "    print(combined_df[preview_cols].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4. Tidying data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Removing rows that aren't jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Where 'illustration of bank notes' in job title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of titles in the dataset:\n",
      "  -                     Wellbeing Personal Trainer\n",
      "  -                     Pharmacy Assistant\n",
      "  -                     Dietetics Assistant\n",
      "  -                     Activities Assistant\n",
      "  -                     Deputy Housekeeping Manager\n",
      "\n",
      "Found 4482 potential matches with 'Illustration of banknote'\n",
      "\n",
      "Sample of matching titles:\n",
      "  -                 Illustration of banknotes\n",
      "  - \n",
      "                Illustration of banknotes\n",
      "\n",
      "Original dataset: 227,210 rows\n",
      "Removed: 4,482 rows\n",
      "Filtered dataset: 222,728 rows\n"
     ]
    }
   ],
   "source": [
    "# Remove rows where title contains \"Illustration of banknotes\" with flexible matching\n",
    "\n",
    "# First check if title column exists\n",
    "if \"title\" not in combined_df.columns:\n",
    "    print(\"Warning: 'title' column not found in the dataset.\")\n",
    "else:\n",
    "    # Display some titles to understand what we're working with\n",
    "    print(\"Sample of titles in the dataset:\")\n",
    "    sample_titles = combined_df[\"title\"].dropna().sample(min(5, len(combined_df))).tolist()\n",
    "    for title in sample_titles:\n",
    "        print(f\"  - {title}\")\n",
    "    \n",
    "    # Count original rows\n",
    "    original_count = len(combined_df)\n",
    "    \n",
    "    # Look at potential matches more flexibly\n",
    "    potential_matches = combined_df[combined_df[\"title\"].str.contains(\"Illustration of banknote\", case=False, na=False)]\n",
    "    print(f\"\\nFound {len(potential_matches)} potential matches with 'Illustration of banknote'\")\n",
    "    \n",
    "    if len(potential_matches) > 0:\n",
    "        print(\"\\nSample of matching titles:\")\n",
    "        for title in potential_matches[\"title\"].unique()[:5]:\n",
    "            print(f\"  - {title}\")\n",
    "    \n",
    "    # Remove rows with flexible matching\n",
    "    filtered_df = combined_df[~combined_df[\"title\"].str.contains(\"Illustration of banknote\", case=False, na=False)]\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nOriginal dataset: {original_count:,} rows\")\n",
    "    print(f\"Removed: {original_count - len(filtered_df):,} rows\")\n",
    "    print(f\"Filtered dataset: {len(filtered_df):,} rows\")\n",
    "    \n",
    "    # Replace original with filtered data\n",
    "    combined_df = filtered_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
