{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Import data from the webscraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 73 webscrape CSV files\n",
      "Reference headers from webscrape_2023-08-15_09-20-02.csv: ['title', 'company', 'location', 'salary', 'description']\n",
      "\n",
      "All files have identical column headers\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Get the parent directory path\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# Find all files in the parent directory matching the webscrape pattern\n",
    "csv_pattern = os.path.join(parent_dir, \"webscrape_*.csv\")\n",
    "csv_files = glob.glob(csv_pattern)\n",
    "\n",
    "# Store column headers for each file\n",
    "all_headers = {}\n",
    "consistent = True\n",
    "first_file = None\n",
    "\n",
    "print(f\"Found {len(csv_files)} webscrape CSV files\")\n",
    "\n",
    "# Loop through each file and get headers\n",
    "for file in csv_files:\n",
    "    filename = os.path.basename(file)\n",
    "    \n",
    "    # Just read the headers without loading entire file\n",
    "    headers = pd.read_csv(file, nrows=0).columns.tolist()\n",
    "    all_headers[filename] = headers\n",
    "    \n",
    "    # Store first file's headers as reference\n",
    "    if first_file is None:\n",
    "        first_file = filename\n",
    "        reference_headers = headers\n",
    "        print(f\"Reference headers from {first_file}: {reference_headers}\")\n",
    "    # Compare current file with reference\n",
    "    elif headers != reference_headers:\n",
    "        consistent = False\n",
    "        print(f\"\\nMISMATCH in {filename}:\")\n",
    "        \n",
    "        # Find and show differences\n",
    "        missing = set(reference_headers) - set(headers)\n",
    "        extra = set(headers) - set(reference_headers)\n",
    "        \n",
    "        if missing:\n",
    "            print(f\"  Missing columns: {missing}\")\n",
    "        if extra:\n",
    "            print(f\"  Extra columns: {extra}\")\n",
    "\n",
    "# Print final result\n",
    "if consistent:\n",
    "    print(\"\\nAll files have identical column headers\")\n",
    "else:\n",
    "    print(\"\\nWarning: Column headers differ between files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. Add date and date-times so we can identify where each job came from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded webscrape_2023-08-15_09-20-02.csv with 3923 rows, scraped on 2023-08-15 09:20:02\n",
      "Loaded webscrape_2023-08-23_09-55-48.csv with 4151 rows, scraped on 2023-08-23 09:55:48\n",
      "Loaded webscrape_2023-08-30_10-13-36.csv with 3897 rows, scraped on 2023-08-30 10:13:36\n",
      "Loaded webscrape_2023-09-07_09-32-27.csv with 4266 rows, scraped on 2023-09-07 09:32:27\n",
      "Loaded webscrape_2023-09-27_11-14-05.csv with 3458 rows, scraped on 2023-09-27 11:14:05\n",
      "Loaded webscrape_2023-10-04_09-34-49.csv with 3712 rows, scraped on 2023-10-04 09:34:49\n",
      "Loaded webscrape_2023-10-11_07-59-43.csv with 3362 rows, scraped on 2023-10-11 07:59:43\n",
      "Loaded webscrape_2023-10-18_09-22-00.csv with 3899 rows, scraped on 2023-10-18 09:22:00\n",
      "Loaded webscrape_2023-11-01_08-31-18.csv with 4629 rows, scraped on 2023-11-01 08:31:18\n",
      "Loaded webscrape_2023-11-08_10-36-53.csv with 1859 rows, scraped on 2023-11-08 10:36:53\n",
      "Loaded webscrape_2023-11-22_10-11-56.csv with 2762 rows, scraped on 2023-11-22 10:11:56\n",
      "Loaded webscrape_2023-11-29_10-05-19.csv with 2931 rows, scraped on 2023-11-29 10:05:19\n",
      "Loaded webscrape_2023-12-06_08-37-21.csv with 3958 rows, scraped on 2023-12-06 08:37:21\n",
      "Loaded webscrape_2023-12-13_22-50-29.csv with 4141 rows, scraped on 2023-12-13 22:50:29\n",
      "Loaded webscrape_2023-12-20_20-54-26.csv with 3775 rows, scraped on 2023-12-20 20:54:26\n",
      "Loaded webscrape_2024-01-03_08-30-45.csv with 3193 rows, scraped on 2024-01-03 08:30:45\n",
      "Loaded webscrape_2024-01-10_15-49-59.csv with 4081 rows, scraped on 2024-01-10 15:49:59\n",
      "Loaded webscrape_2024-01-17_08-29-40.csv with 3630 rows, scraped on 2024-01-17 08:29:40\n",
      "Loaded webscrape_2024-01-31_08-48-04.csv with 3239 rows, scraped on 2024-01-31 08:48:04\n",
      "Loaded webscrape_2024-02-07_09-02-53.csv with 3268 rows, scraped on 2024-02-07 09:02:53\n",
      "Loaded webscrape_2024-02-14_09-27-41.csv with 3427 rows, scraped on 2024-02-14 09:27:41\n",
      "Loaded webscrape_2024-02-21_08-09-31.csv with 3644 rows, scraped on 2024-02-21 08:09:31\n",
      "Loaded webscrape_2024-03-13_08-28-35.csv with 3050 rows, scraped on 2024-03-13 08:28:35\n",
      "Loaded webscrape_2024-03-20_08-11-25.csv with 3491 rows, scraped on 2024-03-20 08:11:25\n",
      "Loaded webscrape_2024-03-27_08-16-53.csv with 2945 rows, scraped on 2024-03-27 08:16:53\n",
      "Loaded webscrape_2024-04-10_08-45-14.csv with 3504 rows, scraped on 2024-04-10 08:45:14\n",
      "Loaded webscrape_2024-04-17_08-08-37.csv with 4330 rows, scraped on 2024-04-17 08:08:37\n",
      "Loaded webscrape_2024-04-24_08-41-09.csv with 3500 rows, scraped on 2024-04-24 08:41:09\n",
      "Loaded webscrape_2024-05-01_09-20-32.csv with 2978 rows, scraped on 2024-05-01 09:20:32\n",
      "Loaded webscrape_2024-05-08_08-47-15.csv with 3199 rows, scraped on 2024-05-08 08:47:15\n",
      "Loaded webscrape_2024-05-15_08-05-44.csv with 2871 rows, scraped on 2024-05-15 08:05:44\n",
      "Loaded webscrape_2024-05-22_11-20-20.csv with 2951 rows, scraped on 2024-05-22 11:20:20\n",
      "Loaded webscrape_2024-05-29_08-28-04.csv with 2248 rows, scraped on 2024-05-29 08:28:04\n",
      "Loaded webscrape_2024-06-05_08-36-42.csv with 2983 rows, scraped on 2024-06-05 08:36:42\n",
      "Loaded webscrape_2024-06-12_08-43-59.csv with 2814 rows, scraped on 2024-06-12 08:43:59\n",
      "Loaded webscrape_2024-06-19_08-37-29.csv with 2501 rows, scraped on 2024-06-19 08:37:29\n",
      "Loaded webscrape_2024-06-26_08-33-33.csv with 2691 rows, scraped on 2024-06-26 08:33:33\n",
      "Loaded webscrape_2024-07-03_08-19-16.csv with 2977 rows, scraped on 2024-07-03 08:19:16\n",
      "Loaded webscrape_2024-07-10_08-04-22.csv with 2637 rows, scraped on 2024-07-10 08:04:22\n",
      "Loaded webscrape_2024-07-17_10-30-02.csv with 2365 rows, scraped on 2024-07-17 10:30:02\n",
      "Loaded webscrape_2024-07-31_10-18-59.csv with 2647 rows, scraped on 2024-07-31 10:18:59\n",
      "Loaded webscrape_2024-08-14_08-46-04.csv with 2732 rows, scraped on 2024-08-14 08:46:04\n",
      "Loaded webscrape_2024-08-21_08-24-07.csv with 2598 rows, scraped on 2024-08-21 08:24:07\n",
      "Loaded webscrape_2024-08-28_08-22-07.csv with 2210 rows, scraped on 2024-08-28 08:22:07\n",
      "Loaded webscrape_2024-09-04_08-34-00.csv with 3159 rows, scraped on 2024-09-04 08:34:00\n",
      "Loaded webscrape_2024-10-09_08-45-47.csv with 2568 rows, scraped on 2024-10-09 08:45:47\n",
      "Loaded webscrape_2024-10-16_08-19-38.csv with 2374 rows, scraped on 2024-10-16 08:19:38\n",
      "Loaded webscrape_2024-10-23_11-48-05.csv with 3016 rows, scraped on 2024-10-23 11:48:05\n",
      "Loaded webscrape_2024-10-30_08-23-59.csv with 5270 rows, scraped on 2024-10-30 08:23:59\n",
      "Loaded webscrape_2024-11-06_08-26-04.csv with 4360 rows, scraped on 2024-11-06 08:26:04\n",
      "Loaded webscrape_2024-11-13_08-59-33.csv with 4969 rows, scraped on 2024-11-13 08:59:33\n",
      "Loaded webscrape_2024-11-20_08-36-01.csv with 2848 rows, scraped on 2024-11-20 08:36:01\n",
      "Loaded webscrape_2024-11-27_08-28-22.csv with 2656 rows, scraped on 2024-11-27 08:28:22\n",
      "Loaded webscrape_2024-12-04_08-23-03.csv with 2501 rows, scraped on 2024-12-04 08:23:03\n",
      "Loaded webscrape_2024-12-11_10-23-04.csv with 2377 rows, scraped on 2024-12-11 10:23:04\n",
      "Loaded webscrape_2024-12-18_09-13-24.csv with 2585 rows, scraped on 2024-12-18 09:13:24\n",
      "Loaded webscrape_2025-01-08_08-43-27.csv with 1949 rows, scraped on 2025-01-08 08:43:27\n",
      "Loaded webscrape_2025-01-15_09-24-29.csv with 3021 rows, scraped on 2025-01-15 09:24:29\n",
      "Loaded webscrape_2025-01-22_08-47-45.csv with 4559 rows, scraped on 2025-01-22 08:47:45\n",
      "Loaded webscrape_2025-01-29_09-16-29.csv with 5272 rows, scraped on 2025-01-29 09:16:29\n",
      "Loaded webscrape_2025-02-05_07-43-17.csv with 5089 rows, scraped on 2025-02-05 07:43:17\n",
      "Loaded webscrape_2025-02-19_08-33-34.csv with 5565 rows, scraped on 2025-02-19 08:33:34\n",
      "Loaded webscrape_2025-02-26_08-20-19.csv with 5122 rows, scraped on 2025-02-26 08:20:19\n",
      "Loaded webscrape_2025-03-05_08-49-43.csv with 4548 rows, scraped on 2025-03-05 08:49:43\n",
      "Loaded webscrape_2025-03-19_09-48-27.csv with 5496 rows, scraped on 2025-03-19 09:48:27\n",
      "Loaded webscrape_2025-03-26_16-52-18.csv with 4479 rows, scraped on 2025-03-26 16:52:18\n",
      "Loaded webscrape_2025-04-02_10-36-11.csv with 5240 rows, scraped on 2025-04-02 10:36:11\n",
      "Loaded webscrape_2025-04-09_08-28-29.csv with 3812 rows, scraped on 2025-04-09 08:28:29\n",
      "Loaded webscrape_2025-04-16_09-28-40.csv with 4872 rows, scraped on 2025-04-16 09:28:40\n",
      "Loaded webscrape_2025-04-23_08-32-59.csv with 4740 rows, scraped on 2025-04-23 08:32:59\n",
      "Loaded webscrape_2025-04-30_08-11-54.csv with 3013 rows, scraped on 2025-04-30 08:11:54\n",
      "Loaded webscrape_2025-05-07_08-38-10.csv with 2505 rows, scraped on 2025-05-07 08:38:10\n",
      "Loaded webscrape_2025-05-07_08-40-42.csv with 2505 rows, scraped on 2025-05-07 08:40:42\n",
      "\n",
      "Processed 73 files\n",
      "\n",
      "Example from webscrape_2023-08-15_09-20-02.csv:\n",
      "  scrape_date     scrape_datetime\n",
      "0  2023-08-15 2023-08-15 09:20:02\n",
      "1  2023-08-15 2023-08-15 09:20:02\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrames with dates extracted from filenames\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Dictionary to store all DataFrames\n",
    "dataframes = {}\n",
    "\n",
    "# Regular expression to extract date and time from filename\n",
    "date_pattern = r'webscrape_(\\d{4}-\\d{2}-\\d{2})_(\\d{2}-\\d{2}-\\d{2})\\.csv'\n",
    "\n",
    "# Process each CSV file\n",
    "for file in csv_files:\n",
    "    filename = os.path.basename(file)\n",
    "    \n",
    "    # Extract date using regex\n",
    "    match = re.search(date_pattern, filename)\n",
    "    if match:\n",
    "        date_str = match.group(1)  # YYYY-MM-DD\n",
    "        time_str = match.group(2).replace('-', ':')  # Convert to HH:MM:SS\n",
    "        datetime_str = f\"{date_str} {time_str}\"\n",
    "        \n",
    "        # Parse to datetime object\n",
    "        file_datetime = datetime.strptime(datetime_str, '%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        # Read the CSV\n",
    "        df = pd.read_csv(file)\n",
    "        \n",
    "        # Add date columns\n",
    "        df['scrape_date'] = file_datetime.date()\n",
    "        df['scrape_datetime'] = file_datetime\n",
    "        \n",
    "        # Store in dictionary\n",
    "        dataframes[filename] = df\n",
    "        \n",
    "        print(f\"Loaded {filename} with {len(df)} rows, scraped on {file_datetime}\")\n",
    "    else:\n",
    "        print(f\"Warning: Couldn't extract date from {filename}\")\n",
    "\n",
    "print(f\"\\nProcessed {len(dataframes)} files\")\n",
    "\n",
    "# Example to verify date columns were added correctly\n",
    "if dataframes:\n",
    "    first_key = list(dataframes.keys())[0]\n",
    "    print(f\"\\nExample from {first_key}:\")\n",
    "    print(dataframes[first_key][['scrape_date', 'scrape_datetime']].head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3. Merge the webscrape CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset created successfully!\n",
      "Total rows: 253,897\n",
      "Total columns: 7\n",
      "\n",
      "Data includes 72 unique dates:\n",
      "  2023-08-15: 3,923 rows\n",
      "  2023-08-23: 4,151 rows\n",
      "  2023-08-30: 3,897 rows\n",
      "  2023-09-07: 4,266 rows\n",
      "  2023-09-27: 3,458 rows\n",
      "  2023-10-04: 3,712 rows\n",
      "  2023-10-11: 3,362 rows\n",
      "  2023-10-18: 3,899 rows\n",
      "  2023-11-01: 4,629 rows\n",
      "  2023-11-08: 1,859 rows\n",
      "  2023-11-22: 2,762 rows\n",
      "  2023-11-29: 2,931 rows\n",
      "  2023-12-06: 3,958 rows\n",
      "  2023-12-13: 4,141 rows\n",
      "  2023-12-20: 3,775 rows\n",
      "  2024-01-03: 3,193 rows\n",
      "  2024-01-10: 4,081 rows\n",
      "  2024-01-17: 3,630 rows\n",
      "  2024-01-31: 3,239 rows\n",
      "  2024-02-07: 3,268 rows\n",
      "  2024-02-14: 3,427 rows\n",
      "  2024-02-21: 3,644 rows\n",
      "  2024-03-13: 3,050 rows\n",
      "  2024-03-20: 3,491 rows\n",
      "  2024-03-27: 2,945 rows\n",
      "  2024-04-10: 3,504 rows\n",
      "  2024-04-17: 4,330 rows\n",
      "  2024-04-24: 3,500 rows\n",
      "  2024-05-01: 2,978 rows\n",
      "  2024-05-08: 3,199 rows\n",
      "  2024-05-15: 2,871 rows\n",
      "  2024-05-22: 2,951 rows\n",
      "  2024-05-29: 2,248 rows\n",
      "  2024-06-05: 2,983 rows\n",
      "  2024-06-12: 2,814 rows\n",
      "  2024-06-19: 2,501 rows\n",
      "  2024-06-26: 2,691 rows\n",
      "  2024-07-03: 2,977 rows\n",
      "  2024-07-10: 2,637 rows\n",
      "  2024-07-17: 2,365 rows\n",
      "  2024-07-31: 2,647 rows\n",
      "  2024-08-14: 2,732 rows\n",
      "  2024-08-21: 2,598 rows\n",
      "  2024-08-28: 2,210 rows\n",
      "  2024-09-04: 3,159 rows\n",
      "  2024-10-09: 2,568 rows\n",
      "  2024-10-16: 2,374 rows\n",
      "  2024-10-23: 3,016 rows\n",
      "  2024-10-30: 5,270 rows\n",
      "  2024-11-06: 4,360 rows\n",
      "  2024-11-13: 4,969 rows\n",
      "  2024-11-20: 2,848 rows\n",
      "  2024-11-27: 2,656 rows\n",
      "  2024-12-04: 2,501 rows\n",
      "  2024-12-11: 2,377 rows\n",
      "  2024-12-18: 2,585 rows\n",
      "  2025-01-08: 1,949 rows\n",
      "  2025-01-15: 3,021 rows\n",
      "  2025-01-22: 4,559 rows\n",
      "  2025-01-29: 5,272 rows\n",
      "  2025-02-05: 5,089 rows\n",
      "  2025-02-19: 5,565 rows\n",
      "  2025-02-26: 5,122 rows\n",
      "  2025-03-05: 4,548 rows\n",
      "  2025-03-19: 5,496 rows\n",
      "  2025-03-26: 4,479 rows\n",
      "  2025-04-02: 5,240 rows\n",
      "  2025-04-09: 3,812 rows\n",
      "  2025-04-16: 4,872 rows\n",
      "  2025-04-23: 4,740 rows\n",
      "  2025-04-30: 3,013 rows\n",
      "  2025-05-07: 5,010 rows\n",
      "\n",
      "Preview of combined dataset:\n",
      "  scrape_date     scrape_datetime  \\\n",
      "0  2023-08-15 2023-08-15 09:20:02   \n",
      "1  2023-08-15 2023-08-15 09:20:02   \n",
      "2  2023-08-15 2023-08-15 09:20:02   \n",
      "3  2023-08-15 2023-08-15 09:20:02   \n",
      "4  2023-08-15 2023-08-15 09:20:02   \n",
      "\n",
      "                                               title  \\\n",
      "0              Average Salary£41,855See More Stats ❯   \n",
      "1                      Advanced Nurse Practitione...   \n",
      "2                                   Registered Nurse   \n",
      "3                      Occupational Therapist Ass...   \n",
      "4                                  An analogue gauge   \n",
      "\n",
      "                                             company  \\\n",
      "0              Receive the newest jobs for this s...   \n",
      "1                      PP ASSOCIATES LTD               \n",
      "2                   EXEMPLAR HEALTH CARE               \n",
      "3              ELYSIUM HEALTHCARE LIMITED        ...   \n",
      "4          Boost your CV\\r\\n        It takes 2 mi...   \n",
      "\n",
      "                                            location  \n",
      "0                                   Create alert      \n",
      "1                              LIVERPOOL, NORTH WEST  \n",
      "2                      ROCHDALE, NORTH WEST, OL16...  \n",
      "3                      WARRINGTON, CHESHIRE, NORT...  \n",
      "4                                                NaN  \n"
     ]
    }
   ],
   "source": [
    "# Merge all DataFrames into one combined dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Check if we have DataFrames to merge\n",
    "if not dataframes:\n",
    "    print(\"No DataFrames to merge!\")\n",
    "else:\n",
    "    # Combine all DataFrames from the dictionary\n",
    "    combined_df = pd.concat(dataframes.values(), ignore_index=True)\n",
    "    \n",
    "    # Basic info about the combined dataset\n",
    "    print(f\"Combined dataset created successfully!\")\n",
    "    print(f\"Total rows: {len(combined_df):,}\")\n",
    "    print(f\"Total columns: {len(combined_df.columns)}\")\n",
    "    \n",
    "    # Show unique scrape dates to verify we have data from different days\n",
    "    unique_dates = combined_df['scrape_date'].unique()\n",
    "    print(f\"\\nData includes {len(unique_dates)} unique dates:\")\n",
    "    for date in sorted(unique_dates):\n",
    "        count = len(combined_df[combined_df['scrape_date'] == date])\n",
    "        print(f\"  {date}: {count:,} rows\")\n",
    "    \n",
    "    # Preview of the combined data\n",
    "    print(\"\\nPreview of combined dataset:\")\n",
    "    # Show date columns first, then a few others\n",
    "    preview_cols = ['scrape_date', 'scrape_datetime'] \n",
    "    # Add a few more columns for preview (adjust as needed)\n",
    "    other_cols = [col for col in combined_df.columns \n",
    "                 if col not in ['scrape_date', 'scrape_datetime']][:3]\n",
    "    preview_cols.extend(other_cols)\n",
    "    \n",
    "    print(combined_df[preview_cols].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4. Tidying data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Removing rows that aren't jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1. Where 'illustration of bank notes' in job title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of titles in the dataset:\n",
      "  -                     First Contact Practitioner (Podiatrist)\n",
      "  -                     Registered Manager\n",
      "  -                     Deputy Nursing Home Manager\n",
      "  -                     Paramedic\n",
      "  -                     Registered Nurse\n",
      "\n",
      "Found 4659 potential matches with 'Illustration of banknote'\n",
      "\n",
      "Sample of matching titles:\n",
      "  -                 Illustration of banknotes\n",
      "  - \n",
      "                Illustration of banknotes\n",
      "\n",
      "Original dataset: 253,897 rows\n",
      "Removed: 4,659 rows\n",
      "Filtered dataset: 249,238 rows\n"
     ]
    }
   ],
   "source": [
    "# Remove rows where title contains \"Illustration of banknotes\" with flexible matching\n",
    "\n",
    "# First check if title column exists\n",
    "if \"title\" not in combined_df.columns:\n",
    "    print(\"Warning: 'title' column not found in the dataset.\")\n",
    "else:\n",
    "    # Display some titles to understand what we're working with\n",
    "    print(\"Sample of titles in the dataset:\")\n",
    "    sample_titles = combined_df[\"title\"].dropna().sample(min(5, len(combined_df))).tolist()\n",
    "    for title in sample_titles:\n",
    "        print(f\"  - {title}\")\n",
    "    \n",
    "    # Count original rows\n",
    "    original_count = len(combined_df)\n",
    "    \n",
    "    # Look at potential matches more flexibly\n",
    "    potential_matches = combined_df[combined_df[\"title\"].str.contains(\"Illustration of banknote\", case=False, na=False)]\n",
    "    print(f\"\\nFound {len(potential_matches)} potential matches with 'Illustration of banknote'\")\n",
    "    \n",
    "    if len(potential_matches) > 0:\n",
    "        print(\"\\nSample of matching titles:\")\n",
    "        for title in potential_matches[\"title\"].unique()[:5]:\n",
    "            print(f\"  - {title}\")\n",
    "    \n",
    "    # Remove rows with flexible matching\n",
    "    filtered_df = combined_df[~combined_df[\"title\"].str.contains(\"Illustration of banknote\", case=False, na=False)]\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nOriginal dataset: {original_count:,} rows\")\n",
    "    print(f\"Removed: {original_count - len(filtered_df):,} rows\")\n",
    "    print(f\"Filtered dataset: {len(filtered_df):,} rows\")\n",
    "    \n",
    "    # Replace original with filtered data\n",
    "    combined_df = filtered_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2. Where company includes 'boost your CV'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 3500 potential matches with 'Boost your CV'\n",
      "\n",
      "Sample of matching company entries:\n",
      "  -         Boost your CV\n",
      "        It takes 2 minutes and it's free.Try ValueMyCV now ❯\n",
      "\n",
      "Original dataset: 249,238 rows\n",
      "Removed: 3,500 rows\n",
      "Filtered dataset: 245,738 rows\n"
     ]
    }
   ],
   "source": [
    "# Remove rows where company contains \"Boost your CV\"\n",
    "\n",
    "# First check if company column exists\n",
    "if \"company\" not in combined_df.columns:\n",
    "    print(\"Warning: 'company' column not found in the dataset.\")\n",
    "else:\n",
    "    # Count original rows\n",
    "    original_count = len(combined_df)\n",
    "    \n",
    "    # Look at potential matches more flexibly\n",
    "    potential_matches = combined_df[combined_df[\"company\"].str.contains(\"Boost your CV\", case=False, na=False)]\n",
    "    print(f\"\\nFound {len(potential_matches)} potential matches with 'Boost your CV'\")\n",
    "    \n",
    "    if len(potential_matches) > 0:\n",
    "        print(\"\\nSample of matching company entries:\")\n",
    "        for company in potential_matches[\"company\"].unique()[:3]:\n",
    "            print(f\"  - {company}\")\n",
    "    \n",
    "    # Remove rows with flexible matching\n",
    "    filtered_df = combined_df[~combined_df[\"company\"].str.contains(\"Boost your CV\", case=False, na=False)]\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nOriginal dataset: {original_count:,} rows\")\n",
    "    print(f\"Removed: {original_count - len(filtered_df):,} rows\")\n",
    "    print(f\"Filtered dataset: {len(filtered_df):,} rows\")\n",
    "    \n",
    "    # Replace original with filtered data\n",
    "    combined_df = filtered_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3. Where company contains 'recent the newest jobs for this search'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 5608 potential matches with 'Receive the newest jobs'\n",
      "\n",
      "Sample of matching company entries:\n",
      "  -             Receive the newest jobs for this search by email:\n",
      "\n",
      "Original dataset: 245,738 rows\n",
      "Removed: 5,608 rows\n",
      "Filtered dataset: 240,130 rows\n"
     ]
    }
   ],
   "source": [
    "# Remove rows where company contains \"Receive the newest jobs\"\n",
    "\n",
    "# First check if company column exists\n",
    "if \"company\" not in combined_df.columns:\n",
    "    print(\"Warning: 'company' column not found in the dataset.\")\n",
    "else:\n",
    "    # Count original rows\n",
    "    original_count = len(combined_df)\n",
    "    \n",
    "    # Look at potential matches with flexible matching\n",
    "    search_phrase = \"Receive the newest jobs\"\n",
    "    potential_matches = combined_df[combined_df[\"company\"].str.contains(search_phrase, case=False, na=False)]\n",
    "    print(f\"\\nFound {len(potential_matches)} potential matches with '{search_phrase}'\")\n",
    "    \n",
    "    if len(potential_matches) > 0:\n",
    "        print(\"\\nSample of matching company entries:\")\n",
    "        for company in potential_matches[\"company\"].unique()[:3]:\n",
    "            print(f\"  - {company}\")\n",
    "    \n",
    "    # Remove rows with flexible matching\n",
    "    filtered_df = combined_df[~combined_df[\"company\"].str.contains(search_phrase, case=False, na=False)]\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nOriginal dataset: {original_count:,} rows\")\n",
    "    print(f\"Removed: {original_count - len(filtered_df):,} rows\")\n",
    "    print(f\"Filtered dataset: {len(filtered_df):,} rows\")\n",
    "    \n",
    "    # Replace original with filtered data\n",
    "    combined_df = filtered_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.1.4. Remove missing salaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Preliminary check*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first check to see if any of these rows actually contain the salary in the job description. Our analysis below shows that:\n",
    "1. All of the rows that have a £ sign in the job title already have a salary in the salary, so\n",
    "2. Missing salaries are not found in the job title, and\n",
    "3. Where a salary is cited in the job title *and* the salary field, these are usually consistent. They differ usually by unit (i.e. £11p/h becomes £11 in the salary field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4,575 rows with '£' in title AND salary field populated\n",
      "Successfully extracted salary from 4564 titles (99.8%)\n",
      "\n",
      "Previously failed cases dataset not available.\n",
      "\n",
      "Still failed to extract from 11 titles\n",
      "\n",
      "Remaining failure cases:\n",
      "Title:                     Vet Surgeon Vacancy - First Opinion Small Animal - Manchester Central Area (to £\n",
      "Salary field: £65\n",
      "\n",
      "Title:                     Vet Surgeon Vacancy - First Opinion Small Animal - Manchester Central Area (to £\n",
      "Salary field: £65\n",
      "\n",
      "Title:                     Post Market Surveillance Manager - Oxford - £Competitive\n",
      "Salary field: £65,000\n",
      "\n",
      "Title:                     Post Market Surveillance Manager - Oxford - £Competitive\n",
      "Salary field: £60,000\n",
      "\n",
      "Title:                     Vet Surgeon Vacancy - First Opinion Small Animal - Manchester Central Area (to £\n",
      "Salary field: £65\n",
      "\n",
      "Title:                     Post Market Surveillance Manager - Oxford - £Competitive\n",
      "Salary field: £60,000\n",
      "\n",
      "Title:                     Post Market Surveillance Manager - Oxford - £Competitive\n",
      "Salary field: £60,000\n",
      "\n",
      "Title:                     Vet Surgeon Vacancy - First Opinion Small Animal - Manchester Central Area (to £\n",
      "Salary field: £65\n",
      "\n",
      "Title:                     Vet Surgeon Vacancy - First Opinion Small Animal - Manchester Central Area (to £\n",
      "Salary field: £65\n",
      "\n",
      "Title:                     Registered Veterinary Nurse - Northwest London - £\n",
      "Salary field: £31\n",
      "\n",
      "Title:                     Travel / Outings Companion Carer £ negotiable per session plus travel expenses.\n",
      "Salary field: £32,397\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Updated salary extraction with improved regex patterns\n",
    "\n",
    "# First check if combined_df exists\n",
    "if \"title\" not in combined_df.columns or \"salary\" not in combined_df.columns:\n",
    "    print(\"Warning: Missing required columns\")\n",
    "else:\n",
    "    # Create DF with rows that have '£' in title AND non-null salary field\n",
    "    pound_in_title = combined_df[\"title\"].str.contains(\"£\", na=False)\n",
    "    has_salary_value = ~combined_df[\"salary\"].isna()\n",
    "    comparison_df = combined_df[pound_in_title & has_salary_value].copy()\n",
    "    \n",
    "    print(f\"Found {len(comparison_df):,} rows with '£' in title AND salary field populated\")\n",
    "    \n",
    "    # Improved function to extract salary from title\n",
    "    def extract_salary_from_title(title):\n",
    "        # Updated patterns with optional space after £\n",
    "        patterns = [\n",
    "            r'£\\s*(\\d{1,3}[,.]?\\d{0,3}[kK]?)', # Basic amounts like £50k, £ 50,000\n",
    "            r'£\\s*(\\d{1,3}[,.]?\\d{0,3})-£?\\s*(\\d{1,3}[,.]?\\d{0,3}[kK]?)', # Ranges like £30-£40k or £ 30-£ 40k\n",
    "            r'£\\s*(\\d{1,3}[,.]?\\d{0,3}[kK]?)[/\\s]*(day|hour|month|year|pa|p\\.a\\.)', # With time period\n",
    "            r'£\\s*(\\d{1,3}[,.]?\\d{0,3})[kK]\\s*-\\s*£?\\s*(\\d{1,3}[,.]?\\d{0,3})[kK]', # Format like £24K-£32K\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, title)\n",
    "            if match:\n",
    "                return match.group(0)\n",
    "        return None\n",
    "    \n",
    "    # Add extracted salary column\n",
    "    comparison_df['title_salary'] = comparison_df['title'].apply(extract_salary_from_title)\n",
    "    \n",
    "    # Count how many titles we could extract salaries from\n",
    "    extracted_count = comparison_df['title_salary'].notna().sum()\n",
    "    print(f\"Successfully extracted salary from {extracted_count} titles ({extracted_count/len(comparison_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Check the previously failed cases\n",
    "    try:\n",
    "        failed_before = failed_extraction_df.copy()\n",
    "        failed_before['updated_title_salary'] = failed_before['title'].apply(extract_salary_from_title)\n",
    "        \n",
    "        newly_extracted = failed_before['updated_title_salary'].notna().sum()\n",
    "        print(f\"\\nOf the {len(failed_before)} previously failed extractions:\")\n",
    "        print(f\"  - Now successfully extracted: {newly_extracted} ({newly_extracted/len(failed_before)*100:.1f}%)\")\n",
    "        \n",
    "        if newly_extracted > 0:\n",
    "            print(\"\\nNewly extracted examples:\")\n",
    "            for _, row in failed_before[failed_before['updated_title_salary'].notna()].iterrows():\n",
    "                print(f\"Title: {row['title']}\")\n",
    "                print(f\"  - Extracted salary: {row['updated_title_salary']}\")\n",
    "                print(f\"  - Salary field: {row['salary']}\")\n",
    "                print()\n",
    "    except NameError:\n",
    "        print(\"\\nPreviously failed cases dataset not available.\")\n",
    "    \n",
    "    # Get current failed extractions\n",
    "    failed_now = comparison_df[comparison_df['title_salary'].isna() & comparison_df['title'].str.contains('£', na=False)]\n",
    "    print(f\"\\nStill failed to extract from {len(failed_now)} titles\")\n",
    "    \n",
    "    if len(failed_now) > 0 and len(failed_now) <= 20:\n",
    "        print(\"\\nRemaining failure cases:\")\n",
    "        for _, row in failed_now.iterrows():\n",
    "            print(f\"Title: {row['title']}\")\n",
    "            print(f\"Salary field: {row['salary']}\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing 4564 rows with successfully extracted title salaries\n",
      "\n",
      "Salary match analysis:\n",
      "  - Exact matches: 1,424 (31.2%)\n",
      "  - Partial matches: 3,140 (68.8%)\n",
      "  - No match: 0 (0.0%)\n",
      "\n",
      "EXAMPLES OF EXACT MATCHES:\n",
      "Title:                     Associate Dentist - £14-15 per UDA\n",
      "  - Title salary: £14\n",
      "  - Field salary: £14\n",
      "\n",
      "Title:                     Associate Dentist - Up to £14 per UDA\n",
      "  - Title salary: £14\n",
      "  - Field salary: £14\n",
      "\n",
      "Title:                     Dental Nurse - £12 per hour\n",
      "  - Title salary: £12\n",
      "  - Field salary: £12\n",
      "\n",
      "\n",
      "EXAMPLES OF PARTIAL MATCHES:\n",
      "Title:                     Newly Qualified Registered Nurse (RGN/RMN/RNLD) – Preston – Nursing Home Setting – £19.00 per hour – Full Training and Support Provided.\n",
      "  - Title salary: £19.00\n",
      "  - Field salary: £19\n",
      "\n",
      "Title:                     Vet Surgeon Vacancy - small animal & exotics - Wirral (to £70K)\n",
      "  - Title salary: £70K\n",
      "  - Field salary: £70\n",
      "\n",
      "Title:                     Newly Qualified Registered Nurse (RGN/RMN/RNLD) – Burnley – Nursing Home Setting – £19.00 per hour – Full Training and Support Provided.\n",
      "  - Title salary: £19.00\n",
      "  - Field salary: £19\n",
      "\n",
      "\n",
      "EXAMPLES OF NO MATCHES:\n"
     ]
    }
   ],
   "source": [
    "# Compare extracted salary from title with salary field\n",
    "\n",
    "# First ensure we have the updated extracted salary data\n",
    "if 'comparison_df' not in locals() or 'title_salary' not in comparison_df.columns:\n",
    "    print(\"Error: Please run the updated salary extraction code first\")\n",
    "else:\n",
    "    # Focus only on rows where we successfully extracted a salary\n",
    "    valid_comparisons = comparison_df[comparison_df['title_salary'].notna()].copy()\n",
    "    \n",
    "    print(f\"Analyzing {len(valid_comparisons)} rows with successfully extracted title salaries\")\n",
    "    \n",
    "    # Function to normalize salary for comparison\n",
    "    def normalize_salary(salary_text):\n",
    "        # Convert to string if not already\n",
    "        salary_str = str(salary_text).lower()\n",
    "        # Remove spaces and common separators\n",
    "        salary_str = salary_str.replace(' ', '').replace(',', '')\n",
    "        return salary_str\n",
    "    \n",
    "    # Add normalized versions of both salary fields\n",
    "    valid_comparisons['norm_title_salary'] = valid_comparisons['title_salary'].apply(normalize_salary)\n",
    "    valid_comparisons['norm_field_salary'] = valid_comparisons['salary'].apply(normalize_salary)\n",
    "    \n",
    "    # Check for matches (using contains rather than exact match)\n",
    "    valid_comparisons['exact_match'] = valid_comparisons['norm_title_salary'] == valid_comparisons['norm_field_salary']\n",
    "    valid_comparisons['partial_match'] = valid_comparisons.apply(\n",
    "        lambda row: (row['norm_title_salary'] in row['norm_field_salary'] or \n",
    "                     row['norm_field_salary'] in row['norm_title_salary']), \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Count match types\n",
    "    exact_matches = valid_comparisons['exact_match'].sum()\n",
    "    partial_matches = valid_comparisons['partial_match'].sum() - exact_matches  # Remove overlap with exact\n",
    "    no_matches = len(valid_comparisons) - exact_matches - partial_matches\n",
    "    \n",
    "    # Calculate percentages\n",
    "    total = len(valid_comparisons)\n",
    "    exact_pct = (exact_matches / total) * 100\n",
    "    partial_pct = (partial_matches / total) * 100\n",
    "    no_match_pct = (no_matches / total) * 100\n",
    "    \n",
    "    # Print match statistics\n",
    "    print(\"\\nSalary match analysis:\")\n",
    "    print(f\"  - Exact matches: {exact_matches:,} ({exact_pct:.1f}%)\")\n",
    "    print(f\"  - Partial matches: {partial_matches:,} ({partial_pct:.1f}%)\")\n",
    "    print(f\"  - No match: {no_matches:,} ({no_match_pct:.1f}%)\")\n",
    "    \n",
    "    # Show examples of each type\n",
    "    print(\"\\nEXAMPLES OF EXACT MATCHES:\")\n",
    "    for _, row in valid_comparisons[valid_comparisons['exact_match']].head(3).iterrows():\n",
    "        print(f\"Title: {row['title']}\")\n",
    "        print(f\"  - Title salary: {row['title_salary']}\")\n",
    "        print(f\"  - Field salary: {row['salary']}\")\n",
    "        print()\n",
    "    \n",
    "    print(\"\\nEXAMPLES OF PARTIAL MATCHES:\")\n",
    "    for _, row in valid_comparisons[valid_comparisons['partial_match'] & ~valid_comparisons['exact_match']].head(3).iterrows():\n",
    "        print(f\"Title: {row['title']}\")\n",
    "        print(f\"  - Title salary: {row['title_salary']}\")\n",
    "        print(f\"  - Field salary: {row['salary']}\")\n",
    "        print()\n",
    "    \n",
    "    print(\"\\nEXAMPLES OF NO MATCHES:\")\n",
    "    for _, row in valid_comparisons[~valid_comparisons['partial_match'] & ~valid_comparisons['exact_match']].head(3).iterrows():\n",
    "        print(f\"Title: {row['title']}\")\n",
    "        print(f\"  - Title salary: {row['title_salary']}\")\n",
    "        print(f\"  - Field salary: {row['salary']}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Removal*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11,301 rows (4.7%) with missing salary values\n",
      "\n",
      "Original dataset: 240,130 rows\n",
      "Removed: 11,301 rows (4.7%)\n",
      "Filtered dataset: 228,829 rows\n",
      "\n",
      "Salary column statistics after cleaning:\n",
      "  - Null values: 0\n",
      "  - Unique values: 32,539\n",
      "\n",
      "Sample salary values:\n",
      "  - £11\n",
      "  - £40,000\n",
      "  - £12\n",
      "  - £35,000\n",
      "  - £45,000\n"
     ]
    }
   ],
   "source": [
    "# Remove rows where 'salary' is missing (now that we've verified salary data quality)\n",
    "\n",
    "# First check if salary column exists\n",
    "if \"salary\" not in combined_df.columns:\n",
    "    print(\"Warning: 'salary' column not found in the dataset.\")\n",
    "else:\n",
    "    # Count original rows\n",
    "    original_count = len(combined_df)\n",
    "    \n",
    "    # Check how many rows have missing salary values\n",
    "    missing_salary_count = combined_df[\"salary\"].isna().sum()\n",
    "    missing_percent = (missing_salary_count / original_count) * 100\n",
    "    \n",
    "    print(f\"Found {missing_salary_count:,} rows ({missing_percent:.1f}%) with missing salary values\")\n",
    "    \n",
    "    # Drop rows where salary is missing\n",
    "    filtered_df = combined_df.dropna(subset=[\"salary\"])\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nOriginal dataset: {original_count:,} rows\")\n",
    "    print(f\"Removed: {missing_salary_count:,} rows ({missing_percent:.1f}%)\")\n",
    "    print(f\"Filtered dataset: {len(filtered_df):,} rows\")\n",
    "    \n",
    "    # Replace original with filtered data\n",
    "    combined_df = filtered_df.copy()\n",
    "    \n",
    "    print(\"\\nSalary column statistics after cleaning:\")\n",
    "    # Check null values (should be zero now)\n",
    "    null_count = combined_df[\"salary\"].isna().sum()\n",
    "    print(f\"  - Null values: {null_count}\")\n",
    "    \n",
    "    # Check unique value count\n",
    "    unique_count = combined_df[\"salary\"].nunique()\n",
    "    print(f\"  - Unique values: {unique_count:,}\")\n",
    "    \n",
    "    # Sample values\n",
    "    print(\"\\nSample salary values:\")\n",
    "    for value in combined_df[\"salary\"].sample(5).tolist():\n",
    "        print(f\"  - {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 350 rows (0.2%) with 'TOP MATCH' as salary\n",
      "\n",
      "Of these 'TOP MATCH' rows:\n",
      "  - 102 rows (29.1%) have '£' in description\n",
      "  - 248 rows (70.9%) don't have '£' in description\n",
      "\n",
      "Example descriptions with '£' (first 100 characters):\n",
      "  - ...ouse Care Home, 81 Dickens Ln, Stockport SK12 1NT £21 - £22.50 Per Hour DOE Clumber House Care Home ...\n",
      "\n",
      "  - ...to Join our quality care company You will be paid £16.00 per hour after training completed (provided...\n",
      "\n",
      "  - ...nths acute clinical experience Min grade value is £40,701. Max grade value is £48,054. provide diete...\n",
      "\n",
      "\n",
      "Example descriptions without '£' (first 100 characters):\n",
      "  -              Please Note - Our organisation can not provide work sponsorship, please do not apply fo...\n",
      "\n",
      "  -             We have an exciting opportunity for a MSK Physiotherapist to join our department name te...\n",
      "\n",
      "  -             Job Title Care Assistant - Days Salary PS12.48ph Hours Part-time [07:00am - 19:00pm] Loc...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analyze 'TOP MATCH' salary rows and check for '£' in description\n",
    "\n",
    "# First check if necessary columns exist\n",
    "if \"salary\" not in combined_df.columns:\n",
    "    print(\"Warning: 'salary' column not found in the dataset.\")\n",
    "elif \"description\" not in combined_df.columns:\n",
    "    print(\"Warning: 'description' column not found in the dataset.\")\n",
    "else:\n",
    "    # Find all rows where salary is 'TOP MATCH'\n",
    "    top_match_mask = combined_df[\"salary\"] == \"TOP MATCH\"\n",
    "    top_match_df = combined_df[top_match_mask]\n",
    "    \n",
    "    top_match_count = len(top_match_df)\n",
    "    top_match_percent = (top_match_count / len(combined_df)) * 100\n",
    "    \n",
    "    print(f\"Found {top_match_count:,} rows ({top_match_percent:.1f}%) with 'TOP MATCH' as salary\")\n",
    "    \n",
    "    # Check how many have '£' in description\n",
    "    has_pound_in_desc = top_match_df[\"description\"].str.contains(\"£\", na=False)\n",
    "    pound_count = has_pound_in_desc.sum()\n",
    "    pound_percent = (pound_count / top_match_count) * 100 if top_match_count > 0 else 0\n",
    "    \n",
    "    print(f\"\\nOf these 'TOP MATCH' rows:\")\n",
    "    print(f\"  - {pound_count:,} rows ({pound_percent:.1f}%) have '£' in description\")\n",
    "    print(f\"  - {top_match_count - pound_count:,} rows ({100 - pound_percent:.1f}%) don't have '£' in description\")\n",
    "    \n",
    "    # Show examples of descriptions with pound signs\n",
    "    if pound_count > 0:\n",
    "        print(\"\\nExample descriptions with '£' (first 100 characters):\")\n",
    "        for desc in top_match_df[has_pound_in_desc][\"description\"].sample(min(3, pound_count)).tolist():\n",
    "            # Find the first occurrence of £ and show context\n",
    "            pound_pos = desc.find('£')\n",
    "            start_pos = max(0, pound_pos - 50)\n",
    "            end_pos = min(len(desc), pound_pos + 50)\n",
    "            context = desc[start_pos:end_pos]\n",
    "            print(f\"  - ...{context}...\")\n",
    "            print()\n",
    "    \n",
    "    # Show examples without pound signs\n",
    "    if top_match_count - pound_count > 0:\n",
    "        print(\"\\nExample descriptions without '£' (first 100 characters):\")\n",
    "        for desc in top_match_df[~has_pound_in_desc][\"description\"].sample(min(3, top_match_count - pound_count)).tolist():\n",
    "            print(f\"  - {desc[:100]}...\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where are we? We found that some of the 'salary' contains 'top match'. I also reflected that in my R script, I've got rid of these. IN some cases, albeit these are still small numbers, there is a salary that can be found and extracted from the job description. So this needs to be sorted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WE also need to do PT/FT/hours, and create annualised equivalients. \n",
    "Town/city/county/region etc.\n",
    "Some standardised employer names (Bupa, NHS, Nuffield etc to makej sure we capture these)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
